{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY2zb-HYLG9c",
        "outputId": "ecb4075b-60c4-47f9-823f-553ce59a58d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library.\n",
        "Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization. bold text"
      ],
      "metadata": {
        "id": "G4PRADHxNPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"This is a sentence\"\n",
        "sentence2 = \"This is another big sentence\""
      ],
      "metadata": {
        "id": "KkmDEOYwLYHx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (\n",
        "    word_tokenize,\n",
        "    wordpunct_tokenize,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer,\n",
        "    MWETokenizer\n",
        ")\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(f'Whitespace tokenization = {sentence1.split()}')\n",
        "\n",
        "print(f'Punctuation-based tokenization = {wordpunct_tokenize(sentence1)}')\n",
        "\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('Martha', 'Jones'))\n",
        "print(f'Multi-word expression (MWE) tokenization = {tokenizer.tokenize(word_tokenize(sentence1))}')\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "print(f'Tweet-rules based tokenization = {tokenizer.tokenize(sentence1)}')\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(f'Default/Treebank tokenization = {tokenizer.tokenize(sentence1)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBtkcSsoLqGp",
        "outputId": "af2dd604-af9d-4254-e4e9-a7afae518cda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace tokenization = ['This', 'is', 'a', 'sentence']\n",
            "Punctuation-based tokenization = ['This', 'is', 'a', 'sentence']\n",
            "Multi-word expression (MWE) tokenization = ['This', 'is', 'a', 'sentence']\n",
            "Tweet-rules based tokenization = ['This', 'is', 'a', 'sentence']\n",
            "Default/Treebank tokenization = ['This', 'is', 'a', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#list of tokenized words\n",
        "token = word_tokenize(sentence2)\n",
        "\n",
        "#stem's of each word\n",
        "stem_words = [stemmer.stem(word) for word in token]\n",
        "\n",
        "#print stemming results\n",
        "for e1, e2 in zip(token, stem_words):\n",
        "    print(e1.ljust(13), '-->', '\\t', e2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7IOAgdNMaj6",
        "outputId": "fe594000-f240-422b-e42f-2ecd4c5e8cf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This          --> \t thi\n",
            "is            --> \t is\n",
            "another       --> \t anoth\n",
            "big           --> \t big\n",
            "sentence      --> \t sentenc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#the stemmer requires a language parameter\n",
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "#list of tokenized words\n",
        "token = word_tokenize(sentence2)\n",
        "\n",
        "#stem's of each word\n",
        "stem_words = [snow_stemmer.stem(word) for word in token]\n",
        "\n",
        "#print stemming results\n",
        "for e1, e2 in zip(token, stem_words):\n",
        "    print(e1.ljust(13), '-->', '\\t', e2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhFoeqadMtlC",
        "outputId": "31f89e58-56a7-4e2f-9e00-e6f236315acf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This          --> \t this\n",
            "is            --> \t is\n",
            "another       --> \t anoth\n",
            "big           --> \t big\n",
            "sentence      --> \t sentenc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "token = word_tokenize(sentence2)\n",
        "\n",
        "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
        "\n",
        "#print stemming results\n",
        "for e1, e2 in zip(token, lemmatized_output):\n",
        "    print(e1.ljust(13), '-->', '\\t', e2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwKI7TqBM2cq",
        "outputId": "1dc64cc0-8268-4659-a52c-05fd6edb6e06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This          --> \t This\n",
            "is            --> \t is\n",
            "another       --> \t another\n",
            "big           --> \t big\n",
            "sentence      --> \t sentence\n"
          ]
        }
      ]
    }
  ]
}